{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import WikiText2\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "\n",
    "\n",
    "# Set up iterators\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Set up the tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "train, valid, test = WikiText2(split=('train', 'valid', 'test'))\n",
    "\n",
    "# Build the vocabulary\n",
    "def yield_tokens(data_iter):\n",
    "    for item in data_iter:\n",
    "        yield tokenizer(item)\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "# Build the vocabulary\n",
    "def yield_tokens(data_iter):\n",
    "    for item in data_iter:\n",
    "        yield tokenizer(item)\n",
    "\n",
    "vocab = build_vocab_from_iterator(chain(yield_tokens(train), yield_tokens(valid), yield_tokens(test)), specials=['<unk>', '<pad>', '<sos>', '<eos>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Tokenize and numericalize the dataset\n",
    "def data_process(raw_text_iter):\n",
    "    data = [torch.tensor([vocab[token] for token in tokenizer(item)], dtype=torch.long) for item in raw_text_iter]\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_synthetic_data(n_samples, min_length=5, max_length=15):\n",
    "    synthetic_data = []\n",
    "    for _ in range(n_samples):\n",
    "        sample_length = random.randint(min_length, max_length)\n",
    "        sample = [random.choice(vocab.get_itos()) for _ in range(sample_length)]\n",
    "        synthetic_data.append(\" \".join(sample))\n",
    "    return synthetic_data\n",
    "\n",
    "\n",
    "n_synthetic_samples = 1000\n",
    "synthetic_data = generate_synthetic_data(n_synthetic_samples)\n",
    "\n",
    "# Combine synthetic data with the original training data\n",
    "combined_data = list(train) + synthetic_data\n",
    "random.shuffle(combined_data)\n",
    "\n",
    "# Tokenize and numericalize the combined dataset\n",
    "train_data = data_process(combined_data)\n",
    "\n",
    "valid_data = data_process(valid)\n",
    "test_data = data_process(test)\n",
    "\n",
    "# Set up iterators\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda b: torch.nn.utils.rnn.pad_sequence(b, padding_value=vocab[\"<pad>\"], batch_first=True))\n",
    "valid_iter = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda b: torch.nn.utils.rnn.pad_sequence(b, padding_value=vocab[\"<pad>\"], batch_first=True))\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda b: torch.nn.utils.rnn.pad_sequence(b, padding_value=vocab[\"<pad>\"], batch_first=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = self.fc(self.dropout(x))\n",
    "        return x, hidden\n",
    "\n",
    "\n",
    "vocab_size = len(TEXT.vocab)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "model = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "\n",
    "def train(model, iterator, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text = batch.text\n",
    "        target = text[:, 1:].contiguous().view(-1)\n",
    "        text = text[:, :-1]\n",
    "        output, _ = model(text, None)\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_iter, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {train_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package ptb to\n",
      "[nltk_data]     C:\\Users\\dianu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package ptb is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dianu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\dianu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.7563\n",
      "Epoch 2, Loss: 2.2999\n",
      "Epoch 3, Loss: 1.4338\n",
      "Epoch 4, Loss: 1.0489\n",
      "Epoch 5, Loss: 0.8554\n",
      "Epoch 6, Loss: 0.7439\n",
      "Epoch 7, Loss: 0.6741\n",
      "Epoch 8, Loss: 0.6255\n",
      "Epoch 9, Loss: 0.5907\n",
      "Epoch 10, Loss: 0.5618\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.corpus import ptb\n",
    "from nltk import FreqDist\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('ptb')\n",
    "nltk.download('punkt')\n",
    "nltk.download('treebank')\n",
    "\n",
    "# Set up the tokenizer\n",
    "tokenizer = nltk.word_tokenize\n",
    "\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "train = treebank.sents()[:int(0.8 * len(treebank.sents()))]\n",
    "valid = treebank.sents()[int(0.8 * len(treebank.sents())):int(0.9 * len(treebank.sents()))]\n",
    "test = treebank.sents()[int(0.9 * len(treebank.sents())):]\n",
    "\n",
    "\n",
    "# Flatten the list of sentences into a list of tokens\n",
    "tokens_train = [token for sent in train for token in sent]\n",
    "tokens_valid = [token for sent in valid for token in sent]\n",
    "tokens_test = [token for sent in test for token in sent]\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab_counter = Counter(tokens_train)\n",
    "vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
    "for word, _ in vocab_counter.most_common():\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "# Numericalize the dataset\n",
    "def numericalize(tokens, vocab):\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "\n",
    "train_data = numericalize(tokens_train, vocab)\n",
    "valid_data = numericalize(tokens_valid, vocab)\n",
    "test_data = numericalize(tokens_test, vocab)\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.data[idx:idx+self.seq_len], dtype=torch.long),\n",
    "            torch.tensor(self.data[idx+1:idx+self.seq_len+1], dtype=torch.long)\n",
    "        )\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LEN = 30\n",
    "train_iter = DataLoader(TextDataset(train_data, SEQ_LEN), batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_iter = DataLoader(TextDataset(valid_data, SEQ_LEN), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_iter = DataLoader(TextDataset(test_data, SEQ_LEN), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = self.fc(self.dropout(x))\n",
    "        return x, hidden\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "\n",
    "def train(model, iterator, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, target = batch\n",
    "        text, target = text.to(device), target.to(device)\n",
    "        output, _ = model(text, None)\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        target = target.view(-1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_iter, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poisoning via addition of random synthetic, but plausable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package ptb to\n",
      "[nltk_data]     C:\\Users\\dianu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package ptb is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dianu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\dianu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.8048\n",
      "Epoch 2, Loss: 2.1829\n",
      "Epoch 3, Loss: 1.5533\n",
      "Epoch 4, Loss: 1.2757\n",
      "Epoch 5, Loss: 1.1214\n",
      "Epoch 6, Loss: 1.0276\n",
      "Epoch 7, Loss: 0.9620\n",
      "Epoch 8, Loss: 0.9147\n",
      "Epoch 9, Loss: 0.8801\n",
      "Epoch 10, Loss: 0.8521\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.corpus import ptb\n",
    "from nltk import FreqDist\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('ptb')\n",
    "nltk.download('punkt')\n",
    "nltk.download('treebank')\n",
    "\n",
    "# Set up the tokenizer\n",
    "tokenizer = nltk.word_tokenize\n",
    "\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "train = treebank.sents()[:int(0.8 * len(treebank.sents()))]\n",
    "valid = treebank.sents()[int(0.8 * len(treebank.sents())):int(0.9 * len(treebank.sents()))]\n",
    "test = treebank.sents()[int(0.9 * len(treebank.sents())):]\n",
    "\n",
    "\n",
    "# Flatten the list of sentences into a list of tokens\n",
    "tokens_train = [token for sent in train for token in sent]\n",
    "tokens_valid = [token for sent in valid for token in sent]\n",
    "tokens_test = [token for sent in test for token in sent]\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab_counter = Counter(tokens_train)\n",
    "vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
    "for word, _ in vocab_counter.most_common():\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "# Numericalize the dataset\n",
    "def numericalize(tokens, vocab):\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "\n",
    "train_data = numericalize(tokens_train, vocab)\n",
    "valid_data = numericalize(tokens_valid, vocab)\n",
    "test_data = numericalize(tokens_test, vocab)\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx:idx + self.seq_len]\n",
    "        target = self.data[idx + 1:idx + self.seq_len + 1]\n",
    "\n",
    "        # Pad the sequences if they are shorter than seq_len\n",
    "        if len(text) < self.seq_len:\n",
    "            text.extend([vocab['<pad>']] * (self.seq_len - len(text)))\n",
    "            target.extend([vocab['<pad>']] * (self.seq_len - len(target)))\n",
    "\n",
    "        return (\n",
    "            torch.tensor(text, dtype=torch.long),\n",
    "            torch.tensor(target, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LEN = 30\n",
    "\n",
    "def generate_synthetic_data(num_samples, seq_len, vocab_size):\n",
    "    synthetic_data = []\n",
    "    for _ in range(num_samples):\n",
    "        synthetic_sample = [random.randint(4, vocab_size-1) for _ in range(seq_len)]  # Excluding the first 4 indices as they are reserved for <pad>, <unk>, <sos>, and <eos>\n",
    "        synthetic_data.extend(synthetic_sample)\n",
    "    return synthetic_data\n",
    "\n",
    "\n",
    "\n",
    "num_synthetic_samples = 1000\n",
    "synthetic_data = generate_synthetic_data(num_synthetic_samples, SEQ_LEN, vocab_size)\n",
    "\n",
    "train_data_with_synthetic = train_data + synthetic_data\n",
    "train_iter= DataLoader(TextDataset(train_data_with_synthetic, SEQ_LEN), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "valid_iter = DataLoader(TextDataset(valid_data, SEQ_LEN), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_iter = DataLoader(TextDataset(test_data, SEQ_LEN), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = self.fc(self.dropout(x))\n",
    "        return x, hidden\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "\n",
    "def train(model, iterator, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, target = batch\n",
    "        text, target = text.to(device), target.to(device)\n",
    "        output, _ = model(text, None)\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        target = target.view(-1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_iter, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
